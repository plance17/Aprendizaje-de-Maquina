{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/plance17/Aprendizaje-de-Maquina/blob/master/Redes_Neuronales_Gu%C3%ADa_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gf6CVPqs4Rqb",
        "outputId": "8e3b8dc4-5c11-4836-e9a1-be1f5da9d4dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.11.1+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.5)\n",
            "Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.10.0+cu111)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0->torchvision) (3.10.0.2)\n"
          ]
        }
      ],
      "source": [
        "pip install torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wygWPdA4t9R",
        "outputId": "21f4e3d0-ea49-4272-9d91-f5d3ab15b368"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchviz in /usr/local/lib/python3.7/dist-packages (0.0.2)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchviz) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchviz) (3.10.0.2)\n"
          ]
        }
      ],
      "source": [
        "pip install torchviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {
        "id": "CtHvfTAvy82b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda, Compose\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sklearn as skl\n",
        "from torchviz import make_dot\n",
        "import pandas as pd\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using {} device\".format(device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWIJzry7BV3E",
        "outputId": "9122d012-3c87-4b89-93a9-aa0260d36d0d"
      },
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "id": "-QqtgSoyIdmd"
      },
      "outputs": [],
      "source": [
        "#Cargo la base de datos de FashionMNIST\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "0pIxBGzgJD9H",
        "outputId": "76894c4e-9262-4a12-9cd8-d87f529c11e7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f036f596750>"
            ]
          },
          "metadata": {},
          "execution_count": 194
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "img=training_data[2][0]\n",
        "\n",
        "#img  \n",
        "\n",
        "plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "#plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "id": "3XCtfBMXJEEB"
      },
      "outputs": [],
      "source": [
        "# Creamos una subclase de Dataset que nos sirva para muestrear imagenes en el entrenamiento del autoencoder\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self,dataset):\n",
        "        self.dataset=dataset\n",
        "    # Redefinimos el método .__len__()\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "    # Redefinimos el método .__getitem__()\n",
        "    def __getitem__(self,i):\n",
        "        image,label=self.dataset[i]\n",
        "        label=torch.flatten(image) # Reescribimos el label original con una version achatada de la imagen.\n",
        "        return image,label\n",
        "\n",
        "training_data = CustomImageDataset(training_data)\n",
        "test_data = CustomImageDataset(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Tamaño de los batches\n",
        "batch_size = 1000\n",
        "\n",
        "#Núm de capas ocultas\n",
        "n = 64\n",
        "\n",
        "#Núm de épocas\n",
        "epochs = 100\n",
        "\n",
        "#dropout\n",
        "p=0.1\n",
        "\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n"
      ],
      "metadata": {
        "id": "6hoZ_KIq_Oz0"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self,n=64,p=0.1):\n",
        "        super(Autoencoder,self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p)\n",
        "        self.linear1 = nn.Linear(28*28,n)\n",
        "        self.linear2 = nn.Linear(n,n)\n",
        "        self.linear3 = nn.Linear(n,28*28)\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.linear1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.linear3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "wFdOMv2T_O5f"
      },
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Autoencoder().to(device)\n",
        "print(model)\n",
        "\n",
        "# Optimizamos los parámetros del modelo.\n",
        "# Para ello, necesitamos definir una funcion error.\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "#Uso como optimizador el Método por el gradiente estocástico(SGD)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5fkFgkS_1Uj",
        "outputId": "afb545de-2aa6-4017-8bf4-c8800b0fe3c2"
      },
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autoencoder(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (linear1): Linear(in_features=784, out_features=64, bias=True)\n",
            "  (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (linear3): Linear(in_features=64, out_features=784, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=[]\n",
        "columns  = [\"n\",\"epoch\",\"batch_size\",\"step\",\"loss_train\",\n",
        "            \"loss_test\"]\n",
        "\n",
        "for t in range(epochs):\n",
        "      print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "        \n",
        "\n",
        "#Entrenamiento        \n",
        "      size = len(train_dataloader.dataset)\n",
        "      model.train()\n",
        "      for batch, (X, y) in enumerate(train_dataloader): #Ingreso la matriz (x,y). La función enumerate agrega una columna inicial (i,x,y) que numera los valores i=0,1,2,......\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 10 == 0:\n",
        "            loss_t, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss_t:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "#Test Grupo de entrenamiento\n",
        "      num_samples = len(train_dataloader.dataset)\n",
        "      num_batches = len(train_dataloader)\n",
        "      model.eval()\n",
        "      avrg_loss_train = 0\n",
        "      with torch.no_grad():\n",
        "        for X, y in train_dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            avrg_loss_train += loss_fn(pred, y).item()\n",
        "            #correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "            #Calculamos el nro de predicciones correctas, y lo acumulamos en el total\n",
        "            #frac_correct += (pred.argmax(1)==y).type(torch.float).sum().item()\n",
        "      #calculamos la perdida total y la fraccion de clasificaciones correctas, y las imprimimos\n",
        "      avrg_loss_train /= num_batches\n",
        "      print(f\"Avg loss(Train): {avrg_loss_train:>8f} \\n\")\n",
        "\n",
        "#Test Grupo de validación\n",
        "      num_samples = len(test_dataloader.dataset)\n",
        "      num_batches = len(test_dataloader)\n",
        "      model.eval()\n",
        "      avrg_loss_test = 0\n",
        "      with torch.no_grad():\n",
        "        for X, y in test_dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            avrg_loss_test += loss_fn(pred, y).item()\n",
        "            #correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "            #Calculamos el nro de predicciones correctas, y lo acumulamos en el total\n",
        "            #frac_correct += (pred.argmax(1)==y).type(torch.float).sum().item()\n",
        "      #calculamos la perdida total y la fraccion de clasificaciones correctas, y las imprimimos\n",
        "      avrg_loss_test /= num_batches\n",
        "      print(f\"Avg loss_test: {avrg_loss_test:>8f} \\n\")\n",
        "\n",
        "\n",
        "      values = [n,t,batch_size,t,avrg_loss_train ,avrg_loss_test]\n",
        "      data.append(dict(zip(columns, values)))\n",
        "\n",
        "\n",
        "model_fname = \"model-Adam-\"+str(n)+\"-\"+str(epochs)+\"-\"+str(1)+\"-\"+str(batch_size)+\".pth\"\n",
        "torch.save(model.state_dict(),model_fname)\n",
        "#Guardamos los datos en pandas\n",
        "model_fname = \"model-Adam-\"+str(n)+\"-\"+str(epochs)+\"-\"+str(1)+\"-\"+str(batch_size)+\".csv\"\n",
        "df = pd.DataFrame()\n",
        "df = df.append(data, True)\n",
        "df.to_csv(model_fname)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emdkTfd2AHT4",
        "outputId": "af678995-f7e0-4c5f-e918-6babf6a42380"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.210276  [    0/60000]\n",
            "loss: 0.207887  [10000/60000]\n",
            "loss: 0.203940  [20000/60000]\n",
            "loss: 0.202797  [30000/60000]\n",
            "loss: 0.204436  [40000/60000]\n",
            "loss: 0.207256  [50000/60000]\n",
            "Avg loss(Train): 0.203274 \n",
            "\n",
            "Avg loss_test: 0.203300 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.202116  [    0/60000]\n",
            "loss: 0.201312  [10000/60000]\n",
            "loss: 0.197878  [20000/60000]\n",
            "loss: 0.197067  [30000/60000]\n",
            "loss: 0.198726  [40000/60000]\n",
            "loss: 0.201322  [50000/60000]\n",
            "Avg loss(Train): 0.197192 \n",
            "\n",
            "Avg loss_test: 0.197210 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.196255  [    0/60000]\n",
            "loss: 0.195386  [10000/60000]\n",
            "loss: 0.191926  [20000/60000]\n",
            "loss: 0.190930  [30000/60000]\n",
            "loss: 0.192285  [40000/60000]\n",
            "loss: 0.194655  [50000/60000]\n",
            "Avg loss(Train): 0.189923 \n",
            "\n",
            "Avg loss_test: 0.189928 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.189470  [    0/60000]\n",
            "loss: 0.188379  [10000/60000]\n",
            "loss: 0.184591  [20000/60000]\n",
            "loss: 0.183343  [30000/60000]\n",
            "loss: 0.184323  [40000/60000]\n",
            "loss: 0.186168  [50000/60000]\n",
            "Avg loss(Train): 0.180399 \n",
            "\n",
            "Avg loss_test: 0.180385 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.180812  [    0/60000]\n",
            "loss: 0.179394  [10000/60000]\n",
            "loss: 0.175067  [20000/60000]\n",
            "loss: 0.173510  [30000/60000]\n",
            "loss: 0.173811  [40000/60000]\n",
            "loss: 0.174821  [50000/60000]\n",
            "Avg loss(Train): 0.167464 \n",
            "\n",
            "Avg loss_test: 0.167426 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.169155  [    0/60000]\n",
            "loss: 0.167236  [10000/60000]\n",
            "loss: 0.162236  [20000/60000]\n",
            "loss: 0.160380  [30000/60000]\n",
            "loss: 0.159909  [40000/60000]\n",
            "loss: 0.160151  [50000/60000]\n",
            "Avg loss(Train): 0.150655 \n",
            "\n",
            "Avg loss_test: 0.150581 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.154139  [    0/60000]\n",
            "loss: 0.151934  [10000/60000]\n",
            "loss: 0.146603  [20000/60000]\n",
            "loss: 0.143953  [30000/60000]\n",
            "loss: 0.143184  [40000/60000]\n",
            "loss: 0.142306  [50000/60000]\n",
            "Avg loss(Train): 0.131228 \n",
            "\n",
            "Avg loss_test: 0.131111 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.137080  [    0/60000]\n",
            "loss: 0.134293  [10000/60000]\n",
            "loss: 0.129145  [20000/60000]\n",
            "loss: 0.127092  [30000/60000]\n",
            "loss: 0.125570  [40000/60000]\n",
            "loss: 0.124723  [50000/60000]\n",
            "Avg loss(Train): 0.112311 \n",
            "\n",
            "Avg loss_test: 0.112149 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.120181  [    0/60000]\n",
            "loss: 0.118262  [10000/60000]\n",
            "loss: 0.113129  [20000/60000]\n",
            "loss: 0.111479  [30000/60000]\n",
            "loss: 0.110502  [40000/60000]\n",
            "loss: 0.109761  [50000/60000]\n",
            "Avg loss(Train): 0.096790 \n",
            "\n",
            "Avg loss_test: 0.096588 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.105893  [    0/60000]\n",
            "loss: 0.104869  [10000/60000]\n",
            "loss: 0.100575  [20000/60000]\n",
            "loss: 0.099028  [30000/60000]\n",
            "loss: 0.098450  [40000/60000]\n",
            "loss: 0.099099  [50000/60000]\n",
            "Avg loss(Train): 0.085671 \n",
            "\n",
            "Avg loss_test: 0.085438 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.096595  [    0/60000]\n",
            "loss: 0.094944  [10000/60000]\n",
            "loss: 0.091827  [20000/60000]\n",
            "loss: 0.091782  [30000/60000]\n",
            "loss: 0.090401  [40000/60000]\n",
            "loss: 0.091593  [50000/60000]\n",
            "Avg loss(Train): 0.078407 \n",
            "\n",
            "Avg loss_test: 0.078152 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.088627  [    0/60000]\n",
            "loss: 0.088663  [10000/60000]\n",
            "loss: 0.085344  [20000/60000]\n",
            "loss: 0.085422  [30000/60000]\n",
            "loss: 0.084876  [40000/60000]\n",
            "loss: 0.085885  [50000/60000]\n",
            "Avg loss(Train): 0.073803 \n",
            "\n",
            "Avg loss_test: 0.073533 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.085001  [    0/60000]\n",
            "loss: 0.084014  [10000/60000]\n",
            "loss: 0.081245  [20000/60000]\n",
            "loss: 0.082186  [30000/60000]\n",
            "loss: 0.081881  [40000/60000]\n",
            "loss: 0.083314  [50000/60000]\n",
            "Avg loss(Train): 0.070908 \n",
            "\n",
            "Avg loss_test: 0.070628 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.081785  [    0/60000]\n",
            "loss: 0.080989  [10000/60000]\n",
            "loss: 0.078666  [20000/60000]\n",
            "loss: 0.080023  [30000/60000]\n",
            "loss: 0.079224  [40000/60000]\n",
            "loss: 0.080474  [50000/60000]\n",
            "Avg loss(Train): 0.069082 \n",
            "\n",
            "Avg loss_test: 0.068795 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.079445  [    0/60000]\n",
            "loss: 0.079263  [10000/60000]\n",
            "loss: 0.077421  [20000/60000]\n",
            "loss: 0.077618  [30000/60000]\n",
            "loss: 0.078070  [40000/60000]\n",
            "loss: 0.079001  [50000/60000]\n",
            "Avg loss(Train): 0.067892 \n",
            "\n",
            "Avg loss_test: 0.067601 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.078113  [    0/60000]\n",
            "loss: 0.078495  [10000/60000]\n",
            "loss: 0.076005  [20000/60000]\n",
            "loss: 0.077032  [30000/60000]\n",
            "loss: 0.076677  [40000/60000]\n",
            "loss: 0.077287  [50000/60000]\n",
            "Avg loss(Train): 0.067106 \n",
            "\n",
            "Avg loss_test: 0.066812 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.077331  [    0/60000]\n",
            "loss: 0.077383  [10000/60000]\n",
            "loss: 0.075434  [20000/60000]\n",
            "loss: 0.076098  [30000/60000]\n",
            "loss: 0.075567  [40000/60000]\n",
            "loss: 0.076961  [50000/60000]\n",
            "Avg loss(Train): 0.066527 \n",
            "\n",
            "Avg loss_test: 0.066230 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.077096  [    0/60000]\n",
            "loss: 0.076187  [10000/60000]\n",
            "loss: 0.074461  [20000/60000]\n",
            "loss: 0.075398  [30000/60000]\n",
            "loss: 0.075259  [40000/60000]\n",
            "loss: 0.076469  [50000/60000]\n",
            "Avg loss(Train): 0.066075 \n",
            "\n",
            "Avg loss_test: 0.065777 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.076026  [    0/60000]\n",
            "loss: 0.075905  [10000/60000]\n",
            "loss: 0.073288  [20000/60000]\n",
            "loss: 0.075310  [30000/60000]\n",
            "loss: 0.074601  [40000/60000]\n",
            "loss: 0.075822  [50000/60000]\n",
            "Avg loss(Train): 0.065753 \n",
            "\n",
            "Avg loss_test: 0.065455 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.075417  [    0/60000]\n",
            "loss: 0.075168  [10000/60000]\n",
            "loss: 0.072736  [20000/60000]\n",
            "loss: 0.074144  [30000/60000]\n",
            "loss: 0.074033  [40000/60000]\n",
            "loss: 0.075134  [50000/60000]\n",
            "Avg loss(Train): 0.065435 \n",
            "\n",
            "Avg loss_test: 0.065136 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.074524  [    0/60000]\n",
            "loss: 0.075735  [10000/60000]\n",
            "loss: 0.072619  [20000/60000]\n",
            "loss: 0.074122  [30000/60000]\n",
            "loss: 0.073470  [40000/60000]\n",
            "loss: 0.074994  [50000/60000]\n",
            "Avg loss(Train): 0.065205 \n",
            "\n",
            "Avg loss_test: 0.064907 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.074195  [    0/60000]\n",
            "loss: 0.074342  [10000/60000]\n",
            "loss: 0.072527  [20000/60000]\n",
            "loss: 0.073589  [30000/60000]\n",
            "loss: 0.072934  [40000/60000]\n",
            "loss: 0.074379  [50000/60000]\n",
            "Avg loss(Train): 0.064993 \n",
            "\n",
            "Avg loss_test: 0.064694 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.073896  [    0/60000]\n",
            "loss: 0.073809  [10000/60000]\n",
            "loss: 0.071945  [20000/60000]\n",
            "loss: 0.073209  [30000/60000]\n",
            "loss: 0.073228  [40000/60000]\n",
            "loss: 0.074043  [50000/60000]\n",
            "Avg loss(Train): 0.064716 \n",
            "\n",
            "Avg loss_test: 0.064418 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.073434  [    0/60000]\n",
            "loss: 0.073605  [10000/60000]\n",
            "loss: 0.071539  [20000/60000]\n",
            "loss: 0.072463  [30000/60000]\n",
            "loss: 0.072134  [40000/60000]\n",
            "loss: 0.073751  [50000/60000]\n",
            "Avg loss(Train): 0.064545 \n",
            "\n",
            "Avg loss_test: 0.064247 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.072648  [    0/60000]\n",
            "loss: 0.073433  [10000/60000]\n",
            "loss: 0.070980  [20000/60000]\n",
            "loss: 0.072374  [30000/60000]\n",
            "loss: 0.071783  [40000/60000]\n",
            "loss: 0.072742  [50000/60000]\n",
            "Avg loss(Train): 0.064297 \n",
            "\n",
            "Avg loss_test: 0.064000 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 0.072849  [    0/60000]\n",
            "loss: 0.073328  [10000/60000]\n",
            "loss: 0.071224  [20000/60000]\n",
            "loss: 0.072382  [30000/60000]\n",
            "loss: 0.072046  [40000/60000]\n",
            "loss: 0.073454  [50000/60000]\n",
            "Avg loss(Train): 0.064093 \n",
            "\n",
            "Avg loss_test: 0.063797 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.072856  [    0/60000]\n",
            "loss: 0.072674  [10000/60000]\n",
            "loss: 0.070510  [20000/60000]\n",
            "loss: 0.071510  [30000/60000]\n",
            "loss: 0.071359  [40000/60000]\n",
            "loss: 0.072472  [50000/60000]\n",
            "Avg loss(Train): 0.063876 \n",
            "\n",
            "Avg loss_test: 0.063581 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.072081  [    0/60000]\n",
            "loss: 0.072426  [10000/60000]\n",
            "loss: 0.070122  [20000/60000]\n",
            "loss: 0.071248  [30000/60000]\n",
            "loss: 0.071004  [40000/60000]\n",
            "loss: 0.072373  [50000/60000]\n",
            "Avg loss(Train): 0.063641 \n",
            "\n",
            "Avg loss_test: 0.063346 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.071422  [    0/60000]\n",
            "loss: 0.071868  [10000/60000]\n",
            "loss: 0.069860  [20000/60000]\n",
            "loss: 0.071169  [30000/60000]\n",
            "loss: 0.070835  [40000/60000]\n",
            "loss: 0.071663  [50000/60000]\n",
            "Avg loss(Train): 0.063461 \n",
            "\n",
            "Avg loss_test: 0.063167 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.071534  [    0/60000]\n",
            "loss: 0.071829  [10000/60000]\n",
            "loss: 0.069032  [20000/60000]\n",
            "loss: 0.070846  [30000/60000]\n",
            "loss: 0.070446  [40000/60000]\n",
            "loss: 0.071106  [50000/60000]\n",
            "Avg loss(Train): 0.063225 \n",
            "\n",
            "Avg loss_test: 0.062933 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "loss: 0.071432  [    0/60000]\n",
            "loss: 0.071334  [10000/60000]\n",
            "loss: 0.069480  [20000/60000]\n",
            "loss: 0.070696  [30000/60000]\n",
            "loss: 0.070508  [40000/60000]\n",
            "loss: 0.072044  [50000/60000]\n",
            "Avg loss(Train): 0.062991 \n",
            "\n",
            "Avg loss_test: 0.062700 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "loss: 0.070352  [    0/60000]\n",
            "loss: 0.071409  [10000/60000]\n",
            "loss: 0.069342  [20000/60000]\n",
            "loss: 0.070406  [30000/60000]\n",
            "loss: 0.070049  [40000/60000]\n",
            "loss: 0.071195  [50000/60000]\n",
            "Avg loss(Train): 0.062755 \n",
            "\n",
            "Avg loss_test: 0.062465 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "loss: 0.070338  [    0/60000]\n",
            "loss: 0.070645  [10000/60000]\n",
            "loss: 0.068987  [20000/60000]\n",
            "loss: 0.069738  [30000/60000]\n",
            "loss: 0.069436  [40000/60000]\n",
            "loss: 0.070317  [50000/60000]\n",
            "Avg loss(Train): 0.062552 \n",
            "\n",
            "Avg loss_test: 0.062263 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "loss: 0.069941  [    0/60000]\n",
            "loss: 0.070359  [10000/60000]\n",
            "loss: 0.068000  [20000/60000]\n",
            "loss: 0.069039  [30000/60000]\n",
            "loss: 0.069271  [40000/60000]\n",
            "loss: 0.070532  [50000/60000]\n",
            "Avg loss(Train): 0.062282 \n",
            "\n",
            "Avg loss_test: 0.061994 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "loss: 0.069770  [    0/60000]\n",
            "loss: 0.070002  [10000/60000]\n",
            "loss: 0.067699  [20000/60000]\n",
            "loss: 0.069174  [30000/60000]\n",
            "loss: 0.068768  [40000/60000]\n",
            "loss: 0.069961  [50000/60000]\n",
            "Avg loss(Train): 0.062027 \n",
            "\n",
            "Avg loss_test: 0.061741 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "loss: 0.069488  [    0/60000]\n",
            "loss: 0.070016  [10000/60000]\n",
            "loss: 0.068011  [20000/60000]\n",
            "loss: 0.068667  [30000/60000]\n",
            "loss: 0.068814  [40000/60000]\n",
            "loss: 0.069773  [50000/60000]\n",
            "Avg loss(Train): 0.061783 \n",
            "\n",
            "Avg loss_test: 0.061498 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "loss: 0.068957  [    0/60000]\n",
            "loss: 0.069871  [10000/60000]\n",
            "loss: 0.067508  [20000/60000]\n",
            "loss: 0.068508  [30000/60000]\n",
            "loss: 0.068124  [40000/60000]\n",
            "loss: 0.069787  [50000/60000]\n",
            "Avg loss(Train): 0.061532 \n",
            "\n",
            "Avg loss_test: 0.061249 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "loss: 0.069046  [    0/60000]\n",
            "loss: 0.069142  [10000/60000]\n",
            "loss: 0.067673  [20000/60000]\n",
            "loss: 0.068381  [30000/60000]\n",
            "loss: 0.067974  [40000/60000]\n",
            "loss: 0.069520  [50000/60000]\n",
            "Avg loss(Train): 0.061281 \n",
            "\n",
            "Avg loss_test: 0.061000 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "loss: 0.068539  [    0/60000]\n",
            "loss: 0.068653  [10000/60000]\n",
            "loss: 0.066316  [20000/60000]\n",
            "loss: 0.067993  [30000/60000]\n",
            "loss: 0.067941  [40000/60000]\n",
            "loss: 0.068376  [50000/60000]\n",
            "Avg loss(Train): 0.061010 \n",
            "\n",
            "Avg loss_test: 0.060730 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "loss: 0.068105  [    0/60000]\n",
            "loss: 0.068932  [10000/60000]\n",
            "loss: 0.066798  [20000/60000]\n",
            "loss: 0.067537  [30000/60000]\n",
            "loss: 0.067631  [40000/60000]\n",
            "loss: 0.068612  [50000/60000]\n",
            "Avg loss(Train): 0.060729 \n",
            "\n",
            "Avg loss_test: 0.060451 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "loss: 0.068206  [    0/60000]\n",
            "loss: 0.068283  [10000/60000]\n",
            "loss: 0.066146  [20000/60000]\n",
            "loss: 0.067122  [30000/60000]\n",
            "loss: 0.067425  [40000/60000]\n",
            "loss: 0.068229  [50000/60000]\n",
            "Avg loss(Train): 0.060439 \n",
            "\n",
            "Avg loss_test: 0.060163 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "loss: 0.067716  [    0/60000]\n",
            "loss: 0.067822  [10000/60000]\n",
            "loss: 0.065688  [20000/60000]\n",
            "loss: 0.067143  [30000/60000]\n",
            "loss: 0.066978  [40000/60000]\n",
            "loss: 0.067888  [50000/60000]\n",
            "Avg loss(Train): 0.060139 \n",
            "\n",
            "Avg loss_test: 0.059865 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "loss: 0.066833  [    0/60000]\n",
            "loss: 0.067331  [10000/60000]\n",
            "loss: 0.065491  [20000/60000]\n",
            "loss: 0.066651  [30000/60000]\n",
            "loss: 0.066434  [40000/60000]\n",
            "loss: 0.067189  [50000/60000]\n",
            "Avg loss(Train): 0.059882 \n",
            "\n",
            "Avg loss_test: 0.059609 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "loss: 0.066652  [    0/60000]\n",
            "loss: 0.067560  [10000/60000]\n",
            "loss: 0.065335  [20000/60000]\n",
            "loss: 0.066769  [30000/60000]\n",
            "loss: 0.065921  [40000/60000]\n",
            "loss: 0.067388  [50000/60000]\n",
            "Avg loss(Train): 0.059611 \n",
            "\n",
            "Avg loss_test: 0.059340 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "loss: 0.066785  [    0/60000]\n",
            "loss: 0.067245  [10000/60000]\n",
            "loss: 0.064737  [20000/60000]\n",
            "loss: 0.066026  [30000/60000]\n",
            "loss: 0.066345  [40000/60000]\n",
            "loss: 0.066904  [50000/60000]\n",
            "Avg loss(Train): 0.059295 \n",
            "\n",
            "Avg loss_test: 0.059026 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "loss: 0.066164  [    0/60000]\n",
            "loss: 0.066827  [10000/60000]\n",
            "loss: 0.064732  [20000/60000]\n",
            "loss: 0.065951  [30000/60000]\n",
            "loss: 0.065711  [40000/60000]\n",
            "loss: 0.066578  [50000/60000]\n",
            "Avg loss(Train): 0.058981 \n",
            "\n",
            "Avg loss_test: 0.058714 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "loss: 0.066032  [    0/60000]\n",
            "loss: 0.066222  [10000/60000]\n",
            "loss: 0.064120  [20000/60000]\n",
            "loss: 0.065357  [30000/60000]\n",
            "loss: 0.065339  [40000/60000]\n",
            "loss: 0.066179  [50000/60000]\n",
            "Avg loss(Train): 0.058741 \n",
            "\n",
            "Avg loss_test: 0.058475 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "loss: 0.065478  [    0/60000]\n",
            "loss: 0.066569  [10000/60000]\n",
            "loss: 0.064138  [20000/60000]\n",
            "loss: 0.065236  [30000/60000]\n",
            "loss: 0.064724  [40000/60000]\n",
            "loss: 0.065664  [50000/60000]\n",
            "Avg loss(Train): 0.058413 \n",
            "\n",
            "Avg loss_test: 0.058149 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "loss: 0.065182  [    0/60000]\n",
            "loss: 0.065939  [10000/60000]\n",
            "loss: 0.063336  [20000/60000]\n",
            "loss: 0.064958  [30000/60000]\n",
            "loss: 0.064840  [40000/60000]\n",
            "loss: 0.065432  [50000/60000]\n",
            "Avg loss(Train): 0.058125 \n",
            "\n",
            "Avg loss_test: 0.057862 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "loss: 0.065060  [    0/60000]\n",
            "loss: 0.065579  [10000/60000]\n",
            "loss: 0.063416  [20000/60000]\n",
            "loss: 0.064622  [30000/60000]\n",
            "loss: 0.064217  [40000/60000]\n",
            "loss: 0.065009  [50000/60000]\n",
            "Avg loss(Train): 0.057798 \n",
            "\n",
            "Avg loss_test: 0.057537 \n",
            "\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "loss: 0.064170  [    0/60000]\n",
            "loss: 0.065335  [10000/60000]\n",
            "loss: 0.063287  [20000/60000]\n",
            "loss: 0.064034  [30000/60000]\n",
            "loss: 0.064235  [40000/60000]\n",
            "loss: 0.064337  [50000/60000]\n",
            "Avg loss(Train): 0.057482 \n",
            "\n",
            "Avg loss_test: 0.057224 \n",
            "\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "loss: 0.064642  [    0/60000]\n",
            "loss: 0.064787  [10000/60000]\n",
            "loss: 0.062810  [20000/60000]\n",
            "loss: 0.064058  [30000/60000]\n",
            "loss: 0.063897  [40000/60000]\n",
            "loss: 0.064410  [50000/60000]\n",
            "Avg loss(Train): 0.057187 \n",
            "\n",
            "Avg loss_test: 0.056930 \n",
            "\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "loss: 0.064193  [    0/60000]\n",
            "loss: 0.064332  [10000/60000]\n",
            "loss: 0.062843  [20000/60000]\n",
            "loss: 0.063996  [30000/60000]\n",
            "loss: 0.063107  [40000/60000]\n",
            "loss: 0.064333  [50000/60000]\n",
            "Avg loss(Train): 0.056890 \n",
            "\n",
            "Avg loss_test: 0.056635 \n",
            "\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "loss: 0.063607  [    0/60000]\n",
            "loss: 0.064066  [10000/60000]\n",
            "loss: 0.062014  [20000/60000]\n",
            "loss: 0.063371  [30000/60000]\n",
            "loss: 0.063252  [40000/60000]\n",
            "loss: 0.063656  [50000/60000]\n",
            "Avg loss(Train): 0.056600 \n",
            "\n",
            "Avg loss_test: 0.056347 \n",
            "\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "loss: 0.063342  [    0/60000]\n",
            "loss: 0.063553  [10000/60000]\n",
            "loss: 0.061690  [20000/60000]\n",
            "loss: 0.062868  [30000/60000]\n",
            "loss: 0.062625  [40000/60000]\n",
            "loss: 0.063235  [50000/60000]\n",
            "Avg loss(Train): 0.056279 \n",
            "\n",
            "Avg loss_test: 0.056027 \n",
            "\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "loss: 0.063175  [    0/60000]\n",
            "loss: 0.063612  [10000/60000]\n",
            "loss: 0.061455  [20000/60000]\n",
            "loss: 0.062760  [30000/60000]\n",
            "loss: 0.062957  [40000/60000]\n",
            "loss: 0.062794  [50000/60000]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "n-mMtL-UVzVw",
        "outputId": "3b0b2e09-3362-4f5a-a19d-1d7a0c217f56"
      },
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c263d4dc-9740-42dd-b694-dafa82a22703\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>n</th>\n",
              "      <th>epoch</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>step</th>\n",
              "      <th>loss_train</th>\n",
              "      <th>loss_test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>64</td>\n",
              "      <td>100</td>\n",
              "      <td>1000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.215298</td>\n",
              "      <td>0.215337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>64</td>\n",
              "      <td>100</td>\n",
              "      <td>1000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.215074</td>\n",
              "      <td>0.215113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>64</td>\n",
              "      <td>100</td>\n",
              "      <td>1000</td>\n",
              "      <td>2</td>\n",
              "      <td>0.214861</td>\n",
              "      <td>0.214899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>64</td>\n",
              "      <td>100</td>\n",
              "      <td>1000</td>\n",
              "      <td>3</td>\n",
              "      <td>0.214657</td>\n",
              "      <td>0.214694</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>64</td>\n",
              "      <td>100</td>\n",
              "      <td>1000</td>\n",
              "      <td>4</td>\n",
              "      <td>0.214462</td>\n",
              "      <td>0.214499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>64</td>\n",
              "      <td>100</td>\n",
              "      <td>1000</td>\n",
              "      <td>95</td>\n",
              "      <td>0.206560</td>\n",
              "      <td>0.206582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>64</td>\n",
              "      <td>100</td>\n",
              "      <td>1000</td>\n",
              "      <td>96</td>\n",
              "      <td>0.206490</td>\n",
              "      <td>0.206512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>64</td>\n",
              "      <td>100</td>\n",
              "      <td>1000</td>\n",
              "      <td>97</td>\n",
              "      <td>0.206420</td>\n",
              "      <td>0.206442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>64</td>\n",
              "      <td>100</td>\n",
              "      <td>1000</td>\n",
              "      <td>98</td>\n",
              "      <td>0.206350</td>\n",
              "      <td>0.206372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>64</td>\n",
              "      <td>100</td>\n",
              "      <td>1000</td>\n",
              "      <td>99</td>\n",
              "      <td>0.206280</td>\n",
              "      <td>0.206302</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c263d4dc-9740-42dd-b694-dafa82a22703')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c263d4dc-9740-42dd-b694-dafa82a22703 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c263d4dc-9740-42dd-b694-dafa82a22703');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     n  epoch  batch_size  step  loss_train  loss_test\n",
              "0   64    100        1000     0    0.215298   0.215337\n",
              "1   64    100        1000     1    0.215074   0.215113\n",
              "2   64    100        1000     2    0.214861   0.214899\n",
              "3   64    100        1000     3    0.214657   0.214694\n",
              "4   64    100        1000     4    0.214462   0.214499\n",
              "..  ..    ...         ...   ...         ...        ...\n",
              "95  64    100        1000    95    0.206560   0.206582\n",
              "96  64    100        1000    96    0.206490   0.206512\n",
              "97  64    100        1000    97    0.206420   0.206442\n",
              "98  64    100        1000    98    0.206350   0.206372\n",
              "99  64    100        1000    99    0.206280   0.206302\n",
              "\n",
              "[100 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t=np.arange(0,100)\n",
        "\n",
        "plt.plot(t,df.loss_train)\n",
        "plt.show"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "G7hIHKrCV8Db",
        "outputId": "98f799fa-ffcf-4f6f-e092-e965138a6eeb"
      },
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function matplotlib.pyplot.show>"
            ]
          },
          "metadata": {},
          "execution_count": 217
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num=2\n",
        "\n",
        "imag=X[num]\n",
        "imag_ori=pred[num]\n",
        "\n",
        "plt.imshow(imag.squeeze(),cmap='gray')\n",
        "plt.imshow(imag_ori.squeeze(),cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "6kUP44tQWr4d",
        "outputId": "39c1d351-9518-4be6-acc4-df1368da3cb4"
      },
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-229-e71d15a27e62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimag_ori\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2649\u001b[0m         \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimlim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimlim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m         resample=resample, url=url, **({\"data\": data} if data is not\n\u001b[0;32m-> 2651\u001b[0;31m         None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2652\u001b[0m     \u001b[0msci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2653\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1563\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5624\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5626\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5627\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5628\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    697\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[1;32m    698\u001b[0m             raise TypeError(\"Invalid shape {} for image data\"\n\u001b[0;32m--> 699\u001b[0;31m                             .format(self._A.shape))\n\u001b[0m\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Invalid shape (784,) for image data"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SWCSjP_7YKft"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Redes Neuronales Guía 3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNZ2hiLPJIqh4drymPKMoMt",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}